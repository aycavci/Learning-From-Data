{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"assignment5.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fm75QTbTvkxP","executionInfo":{"status":"ok","timestamp":1633714579230,"user_tz":-120,"elapsed":184,"user":{"displayName":"Ayça Avcı","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gja9OCNqMIYN28tQGuobiAD_Xj4TZKod2e-qiPM=s64","userId":"07960821357396486348"}},"outputId":"abbe34c2-d82f-4dbb-9c39-1a292de4837c"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"id":"ZkK9yH9wtsBx"},"source":["import random as python_random\n","import json\n","import argparse\n","import time\n","import numpy as np\n","from keras.models import Sequential\n","from keras.layers.core import Dense\n","from keras.layers import Embedding, LSTM, Bidirectional\n","from keras.initializers import Constant\n","from sklearn.metrics import accuracy_score\n","from sklearn.preprocessing import LabelBinarizer\n","from tensorflow.keras.optimizers import SGD, Adam, Adagrad, Adamax, RMSprop\n","from tensorflow.keras.layers import TextVectorization\n","import tensorflow as tf\n","# Make reproducible as much as possible\n","np.random.seed(1234)\n","tf.random.set_seed(1234)\n","python_random.seed(1234)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"3DcC3Ef65rwR","executionInfo":{"status":"ok","timestamp":1633714585596,"user_tz":-120,"elapsed":851,"user":{"displayName":"Ayça Avcı","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gja9OCNqMIYN28tQGuobiAD_Xj4TZKod2e-qiPM=s64","userId":"07960821357396486348"}},"outputId":"93f3dabb-feb8-4d6b-8e09-84993775201a"},"source":["tf.test.gpu_device_name()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/device:GPU:0'"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"G-3uD3oAuKq9"},"source":["def read_corpus(corpus_file):\n","    '''Read in review data set and returns docs and labels'''\n","    documents = []\n","    labels = []\n","    count = 0\n","    with open(corpus_file, encoding='utf-8') as f:\n","        for line in f:\n","            tokens = line.strip()\n","            documents.append(\" \".join(tokens.split()[3:]).strip())\n","            # 6-class problem: books, camera, dvd, health, music, software\n","            labels.append(tokens.split()[0])\n","    for label in labels:\n","      if label == 'software'\n","    return documents, labels\n","\n","def read_embeddings(embeddings_file):\n","    '''Read in word embeddings from file and save as numpy array'''\n","    embeddings = json.load(open(embeddings_file, 'r'))\n","    return {word: np.array(embeddings[word]) for word in embeddings}\n","\n","def get_emb_matrix(voc, emb):\n","    '''Get embedding matrix given vocab and the embeddings'''\n","    num_tokens = len(voc) + 2\n","    word_index = dict(zip(voc, range(len(voc))))\n","    # Bit hacky, get embedding dimension from the word \"the\"\n","    embedding_dim = len(emb[\"the\"])\n","    # Prepare embedding matrix to the correct size\n","    embedding_matrix = np.zeros((num_tokens, embedding_dim))\n","    for word, i in word_index.items():\n","        embedding_vector = emb.get(word)\n","        if embedding_vector is not None:\n","            # Words not found in embedding index will be all-zeros.\n","            embedding_matrix[i] = embedding_vector\n","    # Final matrix with pretrained embeddings that we can feed to embedding layer\n","    return embedding_matrix"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uZKZm2gIubYx"},"source":["def create_model(Y_train, emb_matrix):\n","    '''Create the Keras model to use'''\n","    # Define settings, you might want to create cmd line args for them\n","    learning_rate = 0.01\n","    loss_function = 'categorical_crossentropy'\n","    optim = SGD(learning_rate=learning_rate)\n","    # Take embedding dim and size from emb_matrix\n","    embedding_dim = len(emb_matrix[0])\n","    print(\"Embedding dimension: \", embedding_dim)\n","    num_tokens = len(emb_matrix)\n","    print(\"Num tokens: \", num_tokens)\n","    num_labels = len(Y_train[0])\n","    print(\"Num labels: \", num_labels)\n","    # Now build the model\n","    model = Sequential()\n","    model.add(Embedding(num_tokens, embedding_dim, embeddings_initializer=Constant(emb_matrix), trainable=False))\n","    # Here you should add LSTM layers (and potentially dropout)\n","    # raise NotImplementedError(\"Add LSTM layer(s) here\")\n","    model.add(LSTM(75))\n","    # Ultimately, end with dense layer with softmax\n","    model.add(Dense(units=num_labels, activation=\"softmax\"))\n","    # Compile model using our settings, check for accuracy\n","    model.compile(loss=loss_function, optimizer=optim, metrics=['accuracy'])\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"voUyiwFNugk5"},"source":["def train_model(model, X_train, Y_train, X_dev, Y_dev):\n","    '''Train the model here. Note the different settings you can experiment with!'''\n","    # Potentially change these to cmd line args again\n","    # And yes, don't be afraid to experiment!\n","    verbose = 1\n","    batch_size = 16\n","    epochs = 50\n","    # Early stopping: stop training when there are three consecutive epochs without improving\n","    # It's also possible to monitor the training loss with monitor=\"loss\"\n","    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n","    # Finally fit the model to our data\n","    model.fit(X_train, Y_train, verbose=verbose, epochs=epochs, callbacks=[callback], batch_size=batch_size, validation_data=(X_dev, Y_dev))\n","    # Print final accuracy for the model (clearer overview)\n","    test_set_predict(model, X_dev, Y_dev, \"dev\")\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oCj9y49luixI"},"source":["def test_set_predict(model, X_test, Y_test, ident):\n","    '''Do predictions and measure accuracy on our own test set (that we split off train)'''\n","    # Get predictions using the trained model\n","    Y_pred = model.predict(X_test)\n","    # Finally, convert to numerical labels to get scores with sklearn\n","    Y_pred = np.argmax(Y_pred, axis=1)\n","    # If you have gold data, you can calculate accuracy\n","    Y_test = np.argmax(Y_test, axis=1)\n","    print('Accuracy on own {1} set: {0}'.format(round(accuracy_score(Y_test, Y_pred), 3), ident))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1GGqpM7UuxvB"},"source":["def main():\n","    '''Main function to train and test neural network'''\n","    # Read in the data and embeddings\n","    X_train, Y_train = read_corpus('/content/gdrive/MyDrive/AS5/train.txt')\n","    X_dev, Y_dev = read_corpus('/content/gdrive/MyDrive/AS5/dev.txt')\n","    embeddings = read_embeddings('/content/gdrive/MyDrive/AS5/glove_reviews.json')\n","\n","    # Transform words to indices using a vectorizer\n","    vectorizer = TextVectorization(standardize=None, output_sequence_length=50)\n","    # Use train and dev to create vocab - could also do just train\n","    text_ds = tf.data.Dataset.from_tensor_slices(X_train + X_dev)\n","    vectorizer.adapt(text_ds)\n","    # Dictionary mapping words to idx\n","    voc = vectorizer.get_vocabulary()\n","    emb_matrix = get_emb_matrix(voc, embeddings)\n","\n","    # Transform string labels to one-hot encodings\n","    encoder = LabelBinarizer()\n","    Y_train_bin = encoder.fit_transform(Y_train)  # Use encoder.classes_ to find mapping back\n","    Y_dev_bin = encoder.fit_transform(Y_dev)\n","\n","    # Create model\n","    model = create_model(Y_train, emb_matrix)\n","\n","    # Transform input to vectorized input\n","    X_train_vect = vectorizer(np.array([[s] for s in X_train])).numpy()\n","    X_dev_vect = vectorizer(np.array([[s] for s in X_dev])).numpy()\n","\n","    # Train the model\n","    model = train_model(model, X_train_vect, Y_train_bin, X_dev_vect, Y_dev_bin)\n","\n","    test_file = False\n","    # Do predictions on specified test set\n","    if test_file:\n","        # Read in test set and vectorize\n","        X_test, Y_test = read_corpus('test.txt')\n","        Y_test_bin = encoder.fit_transform(Y_test)\n","        X_test_vect = vectorizer(np.array([[s] for s in X_test])).numpy()\n","        # Finally do the predictions\n","        test_set_predict(model, X_test_vect, Y_test_bin, \"test\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uMOnQuUPxXSo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633717157644,"user_tz":-120,"elapsed":110049,"user":{"displayName":"Ayça Avcı","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gja9OCNqMIYN28tQGuobiAD_Xj4TZKod2e-qiPM=s64","userId":"07960821357396486348"}},"outputId":"982a73a6-8f88-4032-9660-37ddbef09646"},"source":["t0 = time.time()\n","main()\n","train_time = time.time() - t0\n","print(\"Training time: \", train_time)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Embedding dimension:  300\n","Num tokens:  38113\n","Num labels:  6\n","Epoch 1/50\n","313/313 [==============================] - 8s 18ms/step - loss: 1.7466 - accuracy: 0.2656 - val_loss: 1.6796 - val_accuracy: 0.3660\n","Epoch 2/50\n","313/313 [==============================] - 5s 15ms/step - loss: 1.4849 - accuracy: 0.4474 - val_loss: 1.2816 - val_accuracy: 0.5180\n","Epoch 3/50\n","313/313 [==============================] - 5s 16ms/step - loss: 1.0954 - accuracy: 0.5730 - val_loss: 0.8739 - val_accuracy: 0.6740\n","Epoch 4/50\n","313/313 [==============================] - 5s 15ms/step - loss: 0.9058 - accuracy: 0.6532 - val_loss: 0.7202 - val_accuracy: 0.7420\n","Epoch 5/50\n","313/313 [==============================] - 5s 16ms/step - loss: 0.7684 - accuracy: 0.7292 - val_loss: 0.9130 - val_accuracy: 0.6340\n","Epoch 6/50\n","313/313 [==============================] - 5s 15ms/step - loss: 0.6731 - accuracy: 0.7666 - val_loss: 0.6306 - val_accuracy: 0.7740\n","Epoch 7/50\n","313/313 [==============================] - 5s 15ms/step - loss: 0.5737 - accuracy: 0.8056 - val_loss: 0.5498 - val_accuracy: 0.8220\n","Epoch 8/50\n","313/313 [==============================] - 5s 16ms/step - loss: 0.5249 - accuracy: 0.8278 - val_loss: 0.5022 - val_accuracy: 0.8260\n","Epoch 9/50\n","313/313 [==============================] - 5s 16ms/step - loss: 0.4811 - accuracy: 0.8448 - val_loss: 1.5306 - val_accuracy: 0.5780\n","Epoch 10/50\n","313/313 [==============================] - 5s 16ms/step - loss: 0.4776 - accuracy: 0.8406 - val_loss: 0.4578 - val_accuracy: 0.8420\n","Epoch 11/50\n","313/313 [==============================] - 5s 15ms/step - loss: 0.4293 - accuracy: 0.8602 - val_loss: 0.4567 - val_accuracy: 0.8500\n","Epoch 12/50\n","313/313 [==============================] - 5s 15ms/step - loss: 0.4050 - accuracy: 0.8694 - val_loss: 0.7459 - val_accuracy: 0.7260\n","Epoch 13/50\n","313/313 [==============================] - 5s 15ms/step - loss: 0.3888 - accuracy: 0.8764 - val_loss: 0.3514 - val_accuracy: 0.9040\n","Epoch 14/50\n","313/313 [==============================] - 5s 15ms/step - loss: 0.3782 - accuracy: 0.8798 - val_loss: 0.3946 - val_accuracy: 0.8620\n","Epoch 15/50\n","313/313 [==============================] - 5s 15ms/step - loss: 0.3577 - accuracy: 0.8846 - val_loss: 0.3781 - val_accuracy: 0.8780\n","Epoch 16/50\n","313/313 [==============================] - 5s 15ms/step - loss: 0.3567 - accuracy: 0.8838 - val_loss: 0.3266 - val_accuracy: 0.8960\n","Epoch 17/50\n","313/313 [==============================] - 5s 15ms/step - loss: 0.3337 - accuracy: 0.8934 - val_loss: 0.4539 - val_accuracy: 0.8480\n","Epoch 18/50\n","313/313 [==============================] - 5s 15ms/step - loss: 0.3250 - accuracy: 0.8964 - val_loss: 0.3559 - val_accuracy: 0.8940\n","Epoch 19/50\n","313/313 [==============================] - 5s 15ms/step - loss: 0.3170 - accuracy: 0.9002 - val_loss: 0.3636 - val_accuracy: 0.8780\n","Accuracy on own dev set: 0.878\n","Training time:  109.8359706401825\n"]}]},{"cell_type":"code","metadata":{"id":"A9N40juoxuog"},"source":["# !python /content/gdrive/MyDrive/AS5/lfd_assignment5.py --train_file /content/gdrive/MyDrive/AS5/train.txt --dev_file /content/gdrive/MyDrive/AS5/dev.txt --embeddings /content/gdrive/MyDrive/AS5/glove_reviews.json"],"execution_count":null,"outputs":[]}]}